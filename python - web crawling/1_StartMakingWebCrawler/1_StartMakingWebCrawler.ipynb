{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 웹 크롤러 만들기\n",
    "\n",
    "  우리는 항상 매번 반복적인 업무들을 하며 무의미한 시간을 보내는 경우가 생각보다 많습니다. 특히 어떤 트랜드를 조사하거나 동향을 살펴보기 위한 데이터를 수집 하는 경우, 각각의 자료들을 우리가 한땀 한땀 엑셀에 정리해 나가면서 자료를 수집하고, 그에 대한 동향을 살펴보고, 결론을 내기까지에 있어서 \"내가 왜 이걸 하고 있지\"라며 회의적인 생각이 드는 순간이 바로 자료를 수집하는 과정이 아닐까 생각이 됩니다.\n",
    "  \n",
    "  요즘 유튜브나 MOOC같은 온라인 학습 과정 중에 직장인을 대상으로 하는 핫한 교육들이 바로 \"업무 자동화\"인데요, 이는 앞서 말했던 반복적이고 노동집약적인 자료 수집 부분이 가장 큰 역할을 하고 있다고 생각합니다. 또한 이러한 업무적인 관점이 아니라 저 개인적으로는 다양한 상황에 대한 기계학습을 시도해보고 싶은 욕망이 있었는데, 이를 시작하기 위해선 개인적으로 수집한 자료들이 있어야 하고 이를 가능하게 하는 것이 바로 이 웹 크롤러를 제작하는 것이라고 생각했습니다.\n",
    "  \n",
    "  우선 이런 파이썬을 이용한 웹 크롤러 제작에 필요한 것은 두가지 정도가 필요합니다. 특정 url로 요청을 보내 웹 페이지를 요청받을 수 있게 해주는 `requests`와 받아온 웹 페이지의 정보를 우리가 해석하여 활용할 수 있도록 해주는 `BeautyfulSoup`입니다.\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Invalid requirement: '#'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Invalid requirement: '#'\n"
     ]
    }
   ],
   "source": [
    "%pip install requests # install requests!\n",
    "%pip install bs4  # install BeautyfulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    위의 코드에서 bs4라는 패키지를 설치하는데, 해당 패키지 내부에 BeautyfulSoup가 포함되어 있습니다!\n",
    "    \n",
    "    그러면 이제 위의 패키지를 이용해서 특정 키워드에 대한 최신 뉴스들을 받아오는 단순한 크롤러(?)를 만들어 볼껀데요, 우선 네이버 검색에서 요즘 유행(?)하고 있는 \"코로나\"를 키워드로 검색해 봅시다! 그리고 나서 우리는 '최신' '뉴스'에 대한 크롤러를 만들것 이기 때문에 검색창 아래 \"뉴스\"탭을 클릭하시고, \"최신순\"을 클릭해줍니다..\n",
    "    \n",
    "![title](search.png)\n",
    "    \n",
    "    그러면 해당 웹 페이지에서 위의 주소창에 있는 주소가 바로 우리에게 필요한 \"url\"주소 입니다! 이것을 가지고 아래의 코드를 실행하면 다음과 같은 결과를 얻을 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of characters : 559594\n",
      "\n",
      "<!doctype html> <html lang=\"ko\"> <head> <meta charset=\"utf-8\"> <meta name=\"referrer\" content=\"always\">  <meta name=\"format-detection\" content=\"telephone=no,address=no,email=no\"> <meta name=\"viewport\" content=\"width=device-width,initial-scale=1.0,maximum-scale=2.0\"> <meta property=\"og:title\" content=\"코로나 : 네이버 뉴스검색\"/> <meta property=\"og:image\" content=\"https://ssl.pstatic.net/sstatic/search/common/og_v3.png\"> <meta property=\"og:description\" content=\"'코로나'의 네이버 뉴스검색 결과입니다.\"> <meta name=\"description\" lang=\"ko\" content=\"'코로나'의 네이버 뉴스검색 결과입니다.\"> <title>코로나 : 네이버 뉴스검색</title> <link rel=\"shortcut icon\" href=\"https://ssl.pstatic.net/sstatic/search/favicon/favicon_191118_pc.ico\">  <link rel=\"search\" type=\"application/opensearchdescription+xml\" href=\"https://ssl.pstatic.net/sstatic/search/opensearch-description.https.xml\" title=\"Naver\" /><link rel=\"stylesheet\" type=\"text/css\" href=\"https://ssl.pstatic.net/sstatic/search/pc/css/search1_200716.css\"> <link rel=\"stylesheet\" type=\"text/css\" href=\"https://ssl.pstatic.net/sstatic/search/pc/css/search2_200611.css\"> <link rel=\"stylesheet\" type=\"text/css\" href=\"https://ssl.pstatic.net/sstatic/search/pc/css/api_atcmp_200709.css\"><script type=\"text/javascript\"> if (!String.prototype.trim) { String.prototype.trim = function () { return this.replace(/^[\\s\\uFEFF\\xA0]+|[\\s\\uFEFF\\xA0]+$/g, ''); }; } if (!Array.prototype.indexOf) { Array.prototype.indexOf = function(searchElement, fromIndex) { var k; if (this == null) { throw new TypeError('\"this\" is null or not defined'); } var o = Object(this); var len = o.length >>> 0; if (len === 0) { return -1; } var n = fromIndex | 0; if (n >= len) { return -1; } k = Math.max(n >= 0 ? n : len - Math.abs(n), 0); while (k < len) { if (k in o && o[k] === searchElement) { return k; } k++; } return -1; }; } if (typeof(encodeURIComponent) != \"function\") { encodeURIComponent = function (s) { function toHex (n) { var hexchars = \"0123456789ABCDEF\" ; return \"%%\" + hexchars.charAt(n>>4) + hexchars.charAt(n&0xF) ; } var es = \"\" ; for (var i = 0; i < s.length;) { var c = s.charCodeAt(i++) ; if ((c&0xF800) == 0xD800) { var sc = s.charCodeAt(i++) ; c = ((c-0xD800)<<10) + (sc-0xDC00) + 0x10000 ; } if (!(c&~0x7F)) { if ((c>=65&&c<=90) || (c>=97&&c<=122) || (c>=48&&c<=57) || (c>=45&&c<=46) || c==95 || c==33 || c==126 || (c>=39&&c<=42)) es += String.fromCharCode(c) ; else es += toHex(c) ; } else if (!(c&~0x7FF)) es += toHex(0xC0+(c>>6)) + toHex(c&0x3F) ; else if (!(c&~0xFFFF)) es += toHex(0xE0+(c>>12)) + toHex(0x80+(c>>6&0x3F)) + toHex(0x80+(c&0x3F)) ; else es += toHex(0xF0+(c>>18)) + toHex(0x80+(c>>12&0x3F)) + toHex(0x80+(c>>6&0x3F)) + toHex(0x80+(c&0x3F)) ; } return es ; } } naver = window.naver || {}; naver.search = naver.search || {}; var g_D = 0 ; naver.search.error = (function () { var errorList = Array() ; return { add : function (s) { errorList.push(s) ; }, clear : function () { delete errorList ; }, get : function (s) { return errorList ; }, getString : function (d) { if (typeof d === 'undefine\n"
     ]
    }
   ],
   "source": [
    "# 특정 url의 웹페이지의 페이지 정보 불러오기\n",
    "# requests 모듈을 불러온다\n",
    "import requests\n",
    "\n",
    "#내가 찾고자 하는 url을 정의\n",
    "url = \"https://search.naver.com/search.naver?where=news&query=%EC%BD%94%EB%A1%9C%EB%82%98&sm=tab_srt&sort=1&photo=0&field=0&reporter_article=&pd=0&ds=&de=&docid=&nso=so%3Add%2Cp%3Aall%2Ca%3Aall&mynews=0&refresh_start=0&related=0\"\n",
    "\n",
    "# url에 페이지 정보를 요청하여 획득\n",
    "res = requests.get(url)\n",
    "\n",
    "# 획득한 정보 출력\n",
    "print(\"Number of characters : {}\".format(len(res.text)))\n",
    "print()\n",
    "print(res.text[:3000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    `res`라는 변수에 들어가 있는 엄청난 양의 문자들이 나타내는 것은 바로 웹 페이지를 구성하고 있는 HTML 형식의 정보들입니다. 56만개의 문자열을 모두 표현하면 쓸데없이 글이 너무 길어지기 때문에 `print(res.text[:3000])`이런 식으로 3000자만 출력해 보았습니다. 한줄로 나열된 HTML코드들은 불규칙한 문자 덩어리처럼 느껴져 저 안에서 어떠한 정보를 추출해 낸다는 것은 매우 어려운 일이 될 것입니다. 따라서 이를 해결하기 위한 방법 중 하나로, `BeautyfulSoup`라는 패키지를 활용 하고자 합니다.\n",
    "    우선 구체적으로 HTML에 대해서 어느정도 알아두면 좋겠지만, 저도 자세히 알고 있지는 않기 때문에 필요한 부분만 짚고 넘어가도록 하겠습니다. 기본적으로 HTML이라는 것은 웹 페이지에 보여지는 화면에 대한 일종의 설계도입니다. 내가 어떤 페이지를 \"머릿말\", \"본문\" 두가지 요소로 만들고 싶다면 HTML코드는 \n",
    "```html\n",
    "    <head>\n",
    "        머릿말\n",
    "    </head>\n",
    "    <body>\n",
    "        본문\n",
    "    </body>\n",
    "```\n",
    "    이런식으로 구성 될 수 있습니다. 따라서 내가 크롤링 하고 싶은 부분이 웹 페이지를 구성하는 html상에서 어디에 위치하고 있는지만 알고 있다면, html 문서의 규칙을 통해 얻어 올 수 있습니다. 그렇다면 방금 불러온 웹페이지의 정보를 불러와보겠습니다. 이번 포스트에서는 웹 브라우저중 크롬을 사용 활용하겠습니다. 크롬을 킨 다음, 위에서 정의한 url을 복사하여 내가 보고자 하는 페이지로 이동한 다음, F12키를 눌러봅니다.\n",
    "    \n",
    "![chrome_dev_tool](dev_tool.png)\n",
    "\n",
    "    그러면 위 그림과 같이 매우 복잡한 코드를 볼 수 있습니다. 하지만 여기서 모든 html 문서를 뒤져가면서 내가 원하는 부분을 찾아내기는 매우 어려우니, 내가 알아내고자 하는 부분에 마우스를 가져다 놓고 우클릭을 합니다. 그리고 나서 '검사'라는 항목을 클릭하면 바로 내가 찾고자 하는 그 부분의 html이 하이라이트 될 것입니다.\n",
    "    \n",
    "![inspector](inspector.png)\n",
    "    \n",
    "    여기서 마우스를 html 문서 상에서 여러 키워드들 사이를 움직이다 보면 실제 웹 페이지의 다양한 영역이 하이라이트 되는 것을 볼 수 있습니다. 그 후, 내가 받아보고 싶은 정보들은 뉴스들에 대한 정보이기 때문에 해당 부분을 찾아봅니다.\n",
    "    \n",
    "![section](find_sec.png)\n",
    "\n",
    "    그러면 이제 우리는 네이버 뉴스검색에서 실제 뉴스들에 대한 정보들을 담고 있는 부분이 바로 `<div class=\"news mynews section _prs_nws\">`에 해당하는 부분이라는 것을 알았습니다. 그렇다면 해당 되는 부분을 우리는 어떻게 찾을 수 있을까요? 바로 맨 처음에 이야기 했던 그 BeautyfulSoup 패키지를 사용하는 것 입니다. 물론 requests에서 받아온 text정보를 토대로 이를 정보를 추출할 수도 있지만, 해당 작업을 누군가가 이미 해 놓은 패키지가 바로 BeautyfulSoup입니다.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  뉴스 1-10 / 3,883,275건  언론사 선정해당 언론사가 채널 주요기사로 직접 선정한 기사입니다. 뉴스검색 가이드   관련도순 최신순 오래된순   검색결과 자동고침 진행 중 검색결과 자동고침 정지 시작 최신순    [대전 코로나 현황] 22일 확진자 총 8명(202번~209번) 추가 발생 천지일보  1분 전   보내기   22일 대전지역에 코로나19 확진자가 총 8명 추가 발생했다.      방역당국 “숨 내쉴 때도 마스크 써달라…주말이 고비” 국제신문  5분 전   보내기   방역당국은 이번 주말을 향후 코로나19 확산세를 가를 ‘고비’, ‘1차 기로’로 보고 있다. 권 부본부장은 “이번 주말은 수도권에서 시작된 코로나19의 폭발적인 증가를 막을 수 있는 마지노선”이라며 방역수칙을 철저히...      황명선논산시장, 코로나19 확산 방지 강력대응 촉구 금강일보  5분 전   보내기   갖고 코로나19 2차 대유행을 막기 위한 최선의 방책을 정부에 강력 요청했다. 전국시장·군수·구청장 협의회 제공 황명선 논산시장이 정부를 향해 코로나19 감염 차단과 극복을 위해 강력한 대응을 촉구했다. 코로나19가...      서산 현대오일뱅크 직원 30대 남성 코로나19 확진 금강일보  5분 전   보내기   사진은 맹정호 서산시장이 지난 22일 현대오일뱅크 직원 30대 남성 코로나19 확진과 관련 비대면 브리핑을 갖는 모습. 서산시 제공 서산 대산공단 내 현대오일뱅크 직원 30대 남성 A 씨가 코로나19 확진 판정을 받았다....      서산시, 코로나19 확진자 정보공개 확대 논란 금강일보  5분 전   보내기   사진은 맹정호 시장이 지난 19일 코로나19 17번 확진자 발생과 관련 시청 브리핑 룸에서 비대면 브리핑을 하는... 가운데 \"코로나19 확진자의 이동 경로 등 공개범위를 확대하라\"고 지시하고 나서 논란이 예상된다. 특히...      보령시, 첫 코로나19 확진자 발생 금강일보  5분 전   보내기   보령 첫 코로나19 확진자 발생과 관련, 김동일 보령시장이 22일 담화문을 발표하는 장면. 보령시 제공 보령시가 지난 7개월 여간 철저한 방역을 통해 코로나19 청정지역을 유지해왔으나 첫 확진자가 발생, 관계당국과...      맥도날드 서울역점 직원 확진…영업 중단 아시아투데이  5분 전   보내기   맥도날드 서울역점 직원이 코로나19 확진 판정을 받아 서울역점 영업이 중단됐다. 22일 한국맥도날드는 이날 오전 방역당국으로부터 직원 1명이 코로나19 확진 판정을 받았다는 통보를 받고 즉시 매장을 폐쇄했다....      [코로나19] 사랑제일교회-창대교회 가평 휩쓸다 파이낸셜뉴스  6분 전  네이버뉴스   보내기   가평군은 최근 수도권을 중심으로 코로나19 확진자가 급증하는 가운데 지역사회 추가감염 확산을 막기 위한 방역행정에 군민의 적극 협조를 호소했다. 관내에는 서울 사랑제일교회발 코로나19 확진자가 15일 첫 발생...      [현장] “49명 지키세요” 텅텅 빈 웨딩홀…달라진 결혼식 풍경 서울신문  6분 전  네이버뉴스   보내기   또 뷔페식으로 제공되던 식사가 도시락이나 답례품으로 대체되는 등 코로나19 이전과는 확연히 달라진... 했는데, 코로나19 때문인지 40명 정도만 참석했다”고 전했다. 이 관계자는 “사회적 거리두기 단계 상향으로 오늘...      마을과 함께, 행복드림체험학교 시즌 4 종강식 내외뉴스통신  6분 전   보내기   또한 방학으로 인해 생긴 학교 유휴공간을 활용해 코로나19 예방 정부방역 정책에 따라 참가 청소년은... 개발하겠다\"며 \"코로나19 예방 및 방역대책에 맞춘 청소년과 보호자를 위한 마음 방역 프로그램을 확대하겠다\"고...     12345678910다음페이지    뉴스 기사와 댓글로 인한 문제 발생시 24시간 센터로 접수해주세요. 24시간센터 \n"
     ]
    }
   ],
   "source": [
    "# python 패키지 중 하나인 BeautyfulSoup를 활용한 HTML 데이터 추출!\n",
    "# bs4 모듈을 불러온다.\n",
    "import bs4\n",
    "\n",
    "# res.text에 있는 단순 문자열인 html 정보를 넣어 html의 의미를 해석 할 수 있는 bs4 객체를 생성한다.\n",
    "search_bs4 = bs4.BeautifulSoup(res.text)\n",
    "\n",
    "# search_bs4에서 내가 원하는 뉴스들을 갖고 있는 부분을 찾아낸다.\n",
    "news_bs4 = search_bs4.find('div',class_ = 'news mynews section _prs_nws')\n",
    "\n",
    "print(news_bs4.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    `news_bs4.text`에 들어있는 정보들은 바로 현재 화면에서 볼 수 있는 모든 뉴스들에 대한 정보를 담고 있습니다. 가령 현재 페이지에 있는 뉴스들이 몇번째 뉴스인지와 총 몇개의 뉴스가 있는지. 그리고 네이버의 뉴스 검색 가이드 문구, 각 뉴스들의 제목과 요약, 마지막으로 페이지 넘기기 등등등 여기서 위에서 나온 스크린샷과 news_bs4.text의 결과가 다른 것은 스크린샷 찍은 시점과 현재 페이지가 요청받은 시점이 다르기 때문일 수 있습니다.\n",
    "    그러면 우리는 현재 웹 상에 한 페이지에 존재하는 뉴스들에 대한 정보를 크롤링 해 보았습니다. 아직까지는 크롤링을 통해 받아온 정보들을 효율적으로 저장하기에는 매우 불편한 형태로 데이터가 주어져 있습니다. 다음 포스트 때에는 내가 원하는 대로 잘라서 엑셀 상에 저장할 수 있는 방법에 대해서 알아보겠습니다!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
