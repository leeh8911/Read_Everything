{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 웹 크롤러 만들기\n",
    "\n",
    "  우리는 항상 매번 반복적인 업무들을 하며 무의미한 시간을 보내는 경우가 생각보다 많습니다. 특히 어떤 트랜드를 조사하거나 동향을 살펴보기 위한 데이터를 수집 하는 경우, 각각의 자료들을 우리가 한땀 한땀 엑셀에 정리해 나가면서 자료를 수집하고, 그에 대한 동향을 살펴보고, 결론을 내기까지에 있어서 \"내가 왜 이걸 하고 있지\"라며 회의적인 생각이 드는 순간이 바로 자료를 수집하는 과정이 아닐까 생각이 됩니다.\n",
    "  \n",
    "  요즘 유튜브나 MOOC같은 온라인 학습 과정 중에 직장인을 대상으로 하는 핫한 교육들이 바로 \"업무 자동화\"인데요, 이는 앞서 말했던 반복적이고 노동집약적인 자료 수집 부분이 가장 큰 역할을 하고 있다고 생각합니다. 또한 이러한 업무적인 관점이 아니라 저 개인적으로는 다양한 상황에 대한 기계학습을 시도해보고 싶은 욕망이 있었는데, 이를 시작하기 위해선 개인적으로 수집한 자료들이 있어야 하고 이를 가능하게 하는 것이 바로 이 웹 크롤러를 제작하는 것이라고 생각했습니다.\n",
    "  \n",
    "  우선 이런 파이썬을 이용한 웹 크롤러 제작에 필요한 것은 두가지 정도가 필요합니다. 특정 url로 요청을 보내 웹 페이지를 요청받을 수 있게 해주는 `requests`와 받아온 웹 페이지의 정보를 우리가 해석하여 활용할 수 있도록 해주는 `BeautyfulSoup`입니다.\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\leeh8\\anaconda3\\envs\\venv\\lib\\site-packages (2.24.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\leeh8\\anaconda3\\envs\\venv\\lib\\site-packages (from requests) (2020.6.20)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\leeh8\\anaconda3\\envs\\venv\\lib\\site-packages (from requests) (1.25.9)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\leeh8\\anaconda3\\envs\\venv\\lib\\site-packages (from requests) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\leeh8\\anaconda3\\envs\\venv\\lib\\site-packages (from requests) (3.0.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: bs4 in c:\\users\\leeh8\\anaconda3\\envs\\venv\\lib\\site-packages (0.0.1)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\leeh8\\anaconda3\\envs\\venv\\lib\\site-packages (from bs4) (4.9.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\leeh8\\anaconda3\\envs\\venv\\lib\\site-packages (from beautifulsoup4->bs4) (2.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install requests # install requests!\n",
    "%pip install bs4  # install BeautyfulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    위의 코드에서 bs4라는 패키지를 설치하는데, 해당 패키지 내부에 BeautyfulSoup가 포함되어 있습니다!\n",
    "    \n",
    "    그러면 이제 위의 패키지를 이용해서 특정 키워드에 대한 최신 뉴스들을 받아오는 단순한 크롤러(?)를 만들어 볼껀데요, 우선 네이버 검색에서 요즘 유행(?)하고 있는 \"코로나\"를 키워드로 검색해 봅시다! 그리고 나서 우리는 '최신' '뉴스'에 대한 크롤러를 만들것 이기 때문에 검색창 아래 \"뉴스\"탭을 클릭하시고, \"최신순\"을 클릭해줍니다..\n",
    "    \n",
    "![title](src/search.png)\n",
    "    \n",
    "    그러면 해당 웹 페이지에서 위의 주소창에 있는 주소가 바로 우리에게 필요한 \"url\"주소 입니다! 이것을 가지고 아래의 코드를 실행하면 다음과 같은 결과를 얻을 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of characters : 560216\n",
      "\n",
      "<!doctype html> <html lang=\"ko\"> <head> <meta charset=\"utf-8\"> <meta name=\"referrer\" content=\"always\">  <meta name=\"format-detection\" content=\"telephone=no,address=no,email=no\"> <meta name=\"viewport\" content=\"width=device-width,initial-scale=1.0,maximum-scale=2.0\"> <meta property=\"og:title\" content=\"코로나 : 네이버 뉴스검색\"/> <meta property=\"og:image\" content=\"https://ssl.pstatic.net/sstatic/search/common/og_v3.png\"> <meta property=\"og:description\" content=\"'코로나'의 네이버 뉴스검색 결과입니다.\"> <meta name=\"description\" lang=\"ko\" content=\"'코로나'의 네이버 뉴스검색 결과입니다.\"> <title>코로나 : 네이버 뉴스검색</title> <link rel=\"shortcut icon\" href=\"https://ssl.pstatic.net/sstatic/search/favicon/favicon_191118_pc.ico\">  <link rel=\"search\" type=\"application/opensearchdescription+xml\" href=\"https://ssl.pstatic.net/sstatic/search/opensearch-description.https.xml\" title=\"Naver\" /><link rel=\"stylesheet\" type=\"text/css\" href=\"https://ssl.pstatic.net/sstatic/search/pc/css/search1_200716.css\"> <link rel=\"stylesheet\" type=\"text/css\" href=\"https://ssl.pstatic.net/sstatic/search/pc/css/search2_200611.css\"> <link rel=\"stylesheet\" type=\"text/css\" href=\"https://ssl.pstatic.net/sstatic/search/pc/css/api_atcmp_200709.css\"><script type=\"text/javascript\"> if (!String.prototype.trim) { String.prototype.trim = function () { return this.replace(/^[\\s\\uFEFF\\xA0]+|[\\s\\uFEFF\\xA0]+$/g, ''); }; } if (!Array.prototype.indexOf) { Array.prototype.indexOf = function(searchElement, fromIndex) { var k; if (this == null) { throw new TypeError('\"this\" is null or not defined'); } var o = Object(this); var len = o.length >>> 0; if (len === 0) { return -1; } var n = fromIndex | 0; if (n >= len) { return -1; } k = Math.max(n >= 0 ? n : len - Math.abs(n), 0); while (k < len) { if (k in o && o[k] === searchElement) { return k; } k++; } return -1; }; } if (typeof(encodeURIComponent) != \"function\") { encodeURIComponent = function (s) { function toHex (n) { var hexchars = \"0123456789ABCDEF\" ; return \"%%\" + hexchars.charAt(n>>4) + hexchars.charAt(n&0xF) ; } var es = \"\" ; for (var i = 0; i < s.length;) { var c = s.charCodeAt(i++) ; if ((c&0xF800) == 0xD800) { var sc = s.charCodeAt(i++) ; c = ((c-0xD800)<<10) + (sc-0xDC00) + 0x10000 ; } if (!(c&~0x7F)) { if ((c>=65&&c<=90) || (c>=97&&c<=122) || (c>=48&&c<=57) || (c>=45&&c<=46) || c==95 || c==33 || c==126 || (c>=39&&c<=42)) es += String.fromCharCode(c) ; else es += toHex(c) ; } else if (!(c&~0x7FF)) es += toHex(0xC0+(c>>6)) + toHex(c&0x3F) ; else if (!(c&~0xFFFF)) es += toHex(0xE0+(c>>12)) + toHex(0x80+(c>>6&0x3F)) + toHex(0x80+(c&0x3F)) ; else es += toHex(0xF0+(c>>18)) + toHex(0x80+(c>>12&0x3F)) + toHex(0x80+(c>>6&0x3F)) + toHex(0x80+(c&0x3F)) ; } return es ; } } naver = window.naver || {}; naver.search = naver.search || {}; var g_D = 0 ; naver.search.error = (function () { var errorList = Array() ; return { add : function (s) { errorList.push(s) ; }, clear : function () { delete errorList ; }, get : function (s) { return errorList ; }, getString : function (d) { if (typeof d === 'undefine\n"
     ]
    }
   ],
   "source": [
    "# 특정 url의 웹페이지의 페이지 정보 불러오기\n",
    "# requests 모듈을 불러온다\n",
    "import requests\n",
    "\n",
    "#내가 찾고자 하는 url을 정의\n",
    "url = \"https://search.naver.com/search.naver?where=news&query=%EC%BD%94%EB%A1%9C%EB%82%98&sm=tab_srt&sort=1&photo=0&field=0&reporter_article=&pd=0&ds=&de=&docid=&nso=so%3Add%2Cp%3Aall%2Ca%3Aall&mynews=0&refresh_start=0&related=0\"\n",
    "\n",
    "# url에 페이지 정보를 요청하여 획득\n",
    "res = requests.get(url)\n",
    "\n",
    "# 획득한 정보 출력\n",
    "print(\"Number of characters : {}\".format(len(res.text)))\n",
    "print()\n",
    "print(res.text[:3000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    `res`라는 변수에 들어가 있는 엄청난 양의 문자들이 나타내는 것은 바로 웹 페이지를 구성하고 있는 HTML 형식의 정보들입니다. 56만개의 문자열을 모두 표현하면 쓸데없이 글이 너무 길어지기 때문에 `print(res.text[:3000])`이런 식으로 3000자만 출력해 보았습니다. 한줄로 나열된 HTML코드들은 불규칙한 문자 덩어리처럼 느껴져 저 안에서 어떠한 정보를 추출해 낸다는 것은 매우 어려운 일이 될 것입니다. 따라서 이를 해결하기 위한 방법 중 하나로, `BeautyfulSoup`라는 패키지를 활용 하고자 합니다.\n",
    "    우선 구체적으로 HTML에 대해서 어느정도 알아두면 좋겠지만, 저도 자세히 알고 있지는 않기 때문에 필요한 부분만 짚고 넘어가도록 하겠습니다. 기본적으로 HTML이라는 것은 웹 페이지에 보여지는 화면에 대한 일종의 설계도입니다. 내가 어떤 페이지를 \"머릿말\", \"본문\" 두가지 요소로 만들고 싶다면 HTML코드는 \n",
    "```html\n",
    "    <head>\n",
    "        머릿말\n",
    "    </head>\n",
    "    <body>\n",
    "        본문\n",
    "    </body>\n",
    "```\n",
    "    이런식으로 구성 될 수 있습니다. 따라서 내가 크롤링 하고 싶은 부분이 웹 페이지를 구성하는 html상에서 어디에 위치하고 있는지만 알고 있다면, html 문서의 규칙을 통해 얻어 올 수 있습니다. 그렇다면 방금 불러온 웹페이지의 정보를 불러와보겠습니다. 이번 포스트에서는 웹 브라우저중 크롬을 사용 활용하겠습니다. 크롬을 킨 다음, 위에서 정의한 url을 복사하여 내가 보고자 하는 페이지로 이동한 다음, F12키를 눌러봅니다.\n",
    "    \n",
    "![chrome_dev_tool](src/dev_tool.png)\n",
    "\n",
    "    그러면 위 그림과 같이 매우 복잡한 코드를 볼 수 있습니다. 하지만 여기서 모든 html 문서를 뒤져가면서 내가 원하는 부분을 찾아내기는 매우 어려우니, 내가 알아내고자 하는 부분에 마우스를 가져다 놓고 우클릭을 합니다. 그리고 나서 '검사'라는 항목을 클릭하면 바로 내가 찾고자 하는 그 부분의 html이 하이라이트 될 것입니다.\n",
    "    \n",
    "![inspector](src/inspector.png)\n",
    "    \n",
    "    여기서 마우스를 html 문서 상에서 여러 키워드들 사이를 움직이다 보면 실제 웹 페이지의 다양한 영역이 하이라이트 되는 것을 볼 수 있습니다. 그 후, 내가 받아보고 싶은 정보들은 뉴스들에 대한 정보이기 때문에 해당 부분을 찾아봅니다.\n",
    "    \n",
    "![section](src/find_sec.png)\n",
    "\n",
    "    그러면 이제 우리는 네이버 뉴스검색에서 실제 뉴스들에 대한 정보들을 담고 있는 부분이 바로 `<div class=\"news mynews section _prs_nws\">`에 해당하는 부분이라는 것을 알았습니다. 그렇다면 해당 되는 부분을 우리는 어떻게 찾을 수 있을까요? 바로 맨 처음에 이야기 했던 그 BeautyfulSoup 패키지를 사용하는 것 입니다. 물론 requests에서 받아온 text정보를 토대로 이를 정보를 추출할 수도 있지만, 해당 작업을 누군가가 이미 해 놓은 패키지가 바로 BeautyfulSoup입니다.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  뉴스 1-10 / 3,883,067건  언론사 선정해당 언론사가 채널 주요기사로 직접 선정한 기사입니다. 뉴스검색 가이드   관련도순 최신순 오래된순   검색결과 자동고침 진행 중 검색결과 자동고침 정지 시작 최신순     [코로나19] 맥도날드 서울역점 ·근방 스타벅스 각 직원 확진...영업 중단 인터뷰365  2분 전   보내기   자료사진./사진=픽사베이 인터뷰365 이은재 기자 = 서울역사 안에 있는 맥도날드 서울역점과 서울역 건너편에 위치한 스타벅스 서울역동자동점에서 코로나 바이러스 감염증(코로나19) 확진자가 나와 22일 매장 영업이...      경기도 용인시, 하루 새 코로나19 확진자 11명 추가 발생...누적 251명으로 '... 녹색경제신문  3분 전   보내기   경기도 용인시에서 하루 사이 11명(용인 241~251번)의 코로나19 확진자가 추가 발생했다. 용인시 방역당국은... 타 지역이나 관내 확진자의 접촉자 7명도 코로나19 확진환자로 이날 등록됐다. 용인시 방역당국은 이들을...      시흥시청, 53번 확진자 동선 - 대야동 20대 - 50번 가족 등 (코로나19, 22일) 한국면세뉴스  3분 전   보내기   시흥시청이 53번 확진자 동선을 22일 저녁 공개했다. 53번 확진자는 대야동 거주 20대로 50번 확진자 가족이다. 다음은 시청이 알린 동선이다. [시흥시 추가 확진환자 발생 #53] □ 시흥#53 확진환자 정보* 대야동 거주...      청주시청, 코로나 타지역 확진자(인천시 계양구65번) '동선' 아시아뉴스통신  4분 전   보내기   청주시 타지역 코로나 청주시는 22일 코로나19 타지역 확진자 이동경로를 홈페이지와 SNS를 통해 공개했다.  - 인천광역시 계양구 65번 연령: 50대- 거주지: 인천광역시 계양구- 확진일: 2020. 8. 22.(토)...      청주지방법원, 다음 주부터 2주간 휴정 검토 KBS  4분 전  네이버뉴스   보내기   코로나19가 전국으로 확산하면서 청주지방법원이 휴정을 검토하기로 했습니다. 청주지방법원은... 법원행정처는 어제 (21일) 전주지방법원 부장판사가 코로나19에 확진되자, 다음 주 월요일부터 2주 동안 구속이나 가처분...      23일부터 전공의 전원 무기한 파업…정부 '면허 정지' 협박에 오히려 뜨거워진... 메디게이트뉴스  4분 전   보내기   대한전공의협의회는 \"정부는 코로나19 종식 때까지 정책 추진을 전면 중단하고 코로나19 이후 모든 가능성을 열어두고 의료계와 재논의해야만 단체행동을 중단하겠다\"고 밝혔다. 다음은 대한전공의협의회 비상대책위원회...      서울역사편찬원, 『일제 말기 경성지역의 강제동원과 일상』 발간 한국강사신문  4분 전   보내기   다만 현재 코로나19로 인해 서울책방에서의 현장 구매는 불가능하고 서울책방 온라인(https://store.seoul.go.kr)을 통해서만 구매할 수 있다. 한편, 서울역사편찬원에서는 '서울 역사의 취약 분야'를 보강하고 서울 연구자의...      '악의꽃' '미씽' '서울촌놈' 등, 코로나19 여파로 제작 일시 중단 [공식] 티브이데일리  5분 전   보내기   CJ ENM이 신종 코로나바이러스 감염증(코로나19) 확산 방지를 위해 드라마와 예능 촬영을 당분간 중단하기로 결정했다. CJ ENM 측은 22일 \"스튜디오드래곤과 협의 하에 출연진과 제작진의 안전을 위해 tvN과 OCN 드라마...      [속보] 대전서 코로나19 확진자 6명 무더기 발생… 21일 하루만에 11명 확진 충청투데이  6분 전   보내기   대전에서 21일 오후 신종 코로나바이러스 감염증(코로나19) 확진자 6명 추가 발생했다.대전시에 따르면 이날 오후 확진자 6명이 추가되면서 지역 내 코로나19 누적 확진자는 모두 209명으로 늘어났다....      의정부시,하반기 각종 행사 전면 취소 일요신문  6분 전   보내기   의정부시는 최근 수도권발 코로나19 대규모 확산 추세 및 대유행 전망에 따라 시민안전을 고려해 하반기 예정되어 있는 각종 행사를 전면 취소했다. 이에 시는 21일 코로나19 관련 긴급 대책회의를 열어 시의회 의장...     12345678910다음페이지    뉴스 기사와 댓글로 인한 문제 발생시 24시간 센터로 접수해주세요. 24시간센터 \n"
     ]
    }
   ],
   "source": [
    "# python 패키지 중 하나인 BeautyfulSoup를 활용한 HTML 데이터 추출!\n",
    "# bs4 모듈을 불러온다.\n",
    "import bs4\n",
    "\n",
    "# res.text에 있는 단순 문자열인 html 정보를 넣어 html의 의미를 해석 할 수 있는 bs4 객체를 생성한다.\n",
    "search_bs4 = bs4.BeautifulSoup(res.text)\n",
    "\n",
    "# search_bs4에서 내가 원하는 뉴스들을 갖고 있는 부분을 찾아낸다.\n",
    "news_bs4 = search_bs4.find('div',class_ = 'news mynews section _prs_nws')\n",
    "\n",
    "print(news_bs4.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    `news_bs4.text`에 들어있는 정보들은 바로 현재 화면에서 볼 수 있는 모든 뉴스들에 대한 정보를 담고 있습니다. 가령 현재 페이지에 있는 뉴스들이 몇번째 뉴스인지와 총 몇개의 뉴스가 있는지. 그리고 네이버의 뉴스 검색 가이드 문구, 각 뉴스들의 제목과 요약, 마지막으로 페이지 넘기기 등등등 여기서 위에서 나온 스크린샷과 news_bs4.text의 결과가 다른 것은 스크린샷 찍은 시점과 현재 페이지가 요청받은 시점이 다르기 때문일 수 있습니다.\n",
    "    그러면 우리는 현재 웹 상에 한 페이지에 존재하는 뉴스들에 대한 정보를 크롤링 해 보았습니다. 아직까지는 크롤링을 통해 받아온 정보들을 효율적으로 저장하기에는 매우 불편한 형태로 데이터가 주어져 있습니다. 다음 포스트 때에는 내가 원하는 대로 잘라서 엑셀 상에 저장할 수 있는 방법에 대해서 알아보겠습니다!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
